---
layout: post
title:  "理解EM算法"
date:   2024-11-18 09:20:08 +0800
category: "AI"
published: false
---

EM算法，是机器学习中，一个非常重要的算法。为了学习EM算法，我查了不少资料，试图完全理解。

<!--more-->

# 数学铺垫

EM的推导中，用到了Jensen不等式。所以，先复习下Jensen不等式。

## Jensen不等式

Jensen不等式前面有两个概念，一个是凸集，一个是凸函数。

### 凸集

凸集，是定义在实数域上的集合，如果集合中任意两点间的线段，都在集合内，则称该集合为凸集。
从数学角度看，凸集的定义是：

$$
\forall x, y \in C, \forall \lambda \in [0, 1], \lambda x + (1 - \lambda) y \in C
$$

### 凸函数

凸函数，是定义在凸集上的函数，如果函数上任意两点间的线段，都在函数图像的上方，则称该函数为凸函数。

如下图所示：

![20241118145627](https://raw.githubusercontent.com/liwenju0/blog_pictures/main/20241118145627.png)

Note: 凸函数和凹函数，是相对的概念。如果函数f(x)是凸函数，那么-f(x)就是凹函数。从上图看，这个凸函数看起来是“凹”的，这一点需要注意一下。
凸函数的定义是：

$$
f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y)\newline
\forall x, y \in C, \forall \lambda \in [0, 1]
$$

Jensen不等式，是对凸函数这个性质的一个推广，且有多种形式，这里我们重点关注一下随机变量的情况。
其公式为：

$$
f(E[X]) \leq E[f(X)]
$$
其中$f(x)$是凸函数，$X$是随机变量。

这里从直观上理解一下，$f(E[X])$是函数$f(x)$在$X$的期望值处的值，$E[f(X)]$是函数$f(x)$在$X$的所有取值处的值的期望。
如果感觉直接从凸函数的性质直接扩展到Jessen不等式，不好理解，可以将$X$先想像成只有两个值的情况。此时Jessen不等式就跟凸函数的性质形式一致了。
因为期望实际就是加权平均，这一点和凸函数性质的加权平均是一致的。


# EM算法

关于EM算法的推导，可以从最大似然，也可以从联合分布KL散度，后者可以参考苏神的[从最大似然到EM算法：一致的理解方式](https://spaces.ac.cn/archives/5239)。
前者则是大多数资料的推导方式，这里也采用最大似然的方式进行推导。

先用一句话总结一下EM算法要解决的问题：

**含有隐变量的概率模型参数的极大似然估计。**

极大似然估计，是概率论中，估计模型参数的一种方法。其背后的假设是：存在即合理。
假设我们有$N$个样本，记为$x_1, x_2, \ldots, x_N$。
我们假设模型参数为$\theta$，模型可以表示为$P(X|\theta)$，其中$X$是随机变量。
极大似然估计，就是找到一个$\theta$，使得这些样本出现的概率乘积最大。

即：

$$
\begin{align*}
\theta &= \arg \max_{\theta} P(X|\theta) \\
&= \arg \max_{\theta} \prod_{i=1}^N P(x_i|\theta) \\
&= \arg \max_{\theta} \sum_{i=1}^N \log P(x_i|\theta)
\end{align*}
$$

要求$P(X|\theta)$，首先需要知道它的数学形式，这个形式应该是一个概率分布。
我们最熟悉的就是正态分布了。如果用正态分布，那么里面的参数$\theta$就是均值$\mu$和协方差$\Sigma$。
此时我们可以写出$P(X|\theta)$的数学形式：

$$
P(X|\theta) = \sum_{i=1}^N \log\frac{1}{\sqrt{2\pi\Sigma}} e^{-\frac{(x_i - \mu)^2}{2\Sigma}}
$$
求解参数的方法，就是对$P(X|\theta)$求导，然后令导数为0，解出$\mu$和$\Sigma$。
我们演示一下求解$\mu$的导数：

$$
\frac{\partial}{\partial \mu} \sum_{i=1}^N \log P(x_i|\theta) = \sum_{i=1}^N \frac{x_i - \mu}{\Sigma} = 0
$$
求解可得：

$$
\mu = \frac{1}{N} \sum_{i=1}^N x_i
$$

看起来很美好，没有隐变量什么事儿。

但是实际上，咱们这个假设$X$为正态分布，很多时候不好使。
比如，数据是来自多个高斯分布的混合。这里所谓的混合，可以理解成一条数据$x$的概率$P(x)$，是由多个高斯分布混合而成的。每个高斯混合分布贡献一部分概率。不妨假设有$K$个高斯分布，记为$G_1, G_2, \ldots, G_K$。
$P(x)$可以表示为：

$$
P(x) = \sum_{i=1}^K \alpha_i P(x|G_i)
$$

其中$\alpha_i$是每个高斯分布的权重，且满足$\sum_{i=1}^K \alpha_i = 1$。

这里$\alpha_i$是和具体的$x$相关的。每个$x$的$\alpha_i$的和为1。




















