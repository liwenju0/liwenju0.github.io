---
layout: post
title:  "理解EM算法"
date:   2024-11-18 09:20:08 +0800
category: "AI"
published: false
---

EM算法，是机器学习中，一个非常重要的算法。为了学习EM算法，我查了不少资料，试图完全理解。

<!--more-->

# 数学铺垫

EM的推导中，用到了Jensen不等式。所以，先复习下Jensen不等式。

## Jensen不等式

Jensen不等式前面有两个概念，一个是凸集，一个是凸函数。

### 凸集

凸集，是定义在实数域上的集合，如果集合中任意两点间的线段，都在集合内，则称该集合为凸集。
从数学角度看，凸集的定义是：

$$
\forall x, y \in C, \forall \lambda \in [0, 1], \lambda x + (1 - \lambda) y \in C
$$

### 凸函数

凸函数，是定义在凸集上的函数，如果函数上任意两点间的线段，都在函数图像的上方，则称该函数为凸函数。

如下图所示：

![20241118145627](https://raw.githubusercontent.com/liwenju0/blog_pictures/main/20241118145627.png)

Note: 凸函数和凹函数，是相对的概念。如果函数f(x)是凸函数，那么-f(x)就是凹函数。从上图看，这个凸函数看起来是“凹”的，这一点需要注意一下。
凸函数的定义是：

$$
f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y)\newline
\forall x, y \in C, \forall \lambda \in [0, 1]
$$

Jensen不等式，是对凸函数这个性质的一个推广，且有多种形式，这里我们重点关注一下随机变量的情况。
其公式为：

$$
f(E[X]) \leq E[f(X)]
$$
其中$f(x)$是凸函数，$X$是随机变量。

这里从直观上理解一下，$f(E[X])$是函数$f(x)$在$X$的期望值处的值，$E[f(X)]$是函数$f(x)$在$X$的所有取值处的值的期望。
如果感觉直接从凸函数的性质直接扩展到Jessen不等式，不好理解，可以将$X$先想像成只有两个值的情况。此时Jessen不等式就跟凸函数的性质形式一致了。
因为期望实际就是加权平均，这一点和凸函数性质的加权平均是一致的。


# EM算法

## 隐变量

关于EM算法的推导，可以从最大似然，也可以从联合分布KL散度，后者可以参考苏神的[从最大似然到EM算法：一致的理解方式](https://spaces.ac.cn/archives/5239)。
前者则是大多数资料的推导方式，这里也采用最大似然的方式进行推导。

先用一句话总结一下EM算法要解决的问题：

**含有隐变量的概率模型参数的极大似然估计。**

极大似然估计，是概率论中，估计模型参数的一种方法。其背后的假设是：存在即合理。
假设我们有$N$个样本，记为$x_1, x_2, \ldots, x_N$。
我们假设模型参数为$\theta$，模型可以表示为$P(X|\theta)$，其中$X$是随机变量。
极大似然估计，就是找到一个$\theta$，使得这些样本出现的概率乘积最大。

即：

$$
\begin{align*}
\theta &= \arg \max_{\theta} P(X|\theta) \\
&= \arg \max_{\theta} \prod_{i=1}^N P(x_i|\theta) \\
&= \arg \max_{\theta} \sum_{i=1}^N \log P(x_i|\theta)
\end{align*}
$$

要求$P(X|\theta)$，首先需要知道它的数学形式，这个形式应该是一个概率分布。
我们最熟悉的就是正态分布了。如果用正态分布，那么里面的参数$\theta$就是均值$\mu$和协方差$\Sigma$。
此时我们可以写出$P(X|\theta)$的数学形式：

$$
P(X|\theta) = \sum_{i=1}^N \log\frac{1}{\sqrt{2\pi\Sigma}} e^{-\frac{(x_i - \mu)^2}{2\Sigma}}
$$
求解参数的方法，就是对$P(X|\theta)$求导，然后令导数为0，解出$\mu$和$\Sigma$。
我们演示一下求解$\mu$的导数：

$$
\frac{\partial}{\partial \mu} \sum_{i=1}^N \log P(x_i|\theta) = \sum_{i=1}^N \frac{x_i - \mu}{\Sigma} = 0
$$
求解可得：

$$
\mu = \frac{1}{N} \sum_{i=1}^N x_i
$$

看起来很美好，是不是？

（下面将会引入隐变量，请仔细体会隐变量的引入过程）

但很多时候，这样的假设并不成立。考虑我们有一些身高数据，$\{170, 161, 172, 173, 164, 175, 176, 167, \ldots, 179, 180\}$。
此时不能简单假设这些身高数据来自一个正态分布。男生和女生身高数据，应该来自两个不同的正态分布。
所以我们这些数据实际上是来自两个正态分布的混合。

请仔细体会，我们现在从一个身高的数据分布中进行抽样，得到了上面的数据。我们知道**每条具体抽样**到的身高数据，要么来自男生，要么来自女生。 这个“身高的数据分布”背后实际上对应着一个男生女生分布。怎么理解这里的对应？我们可以理解，每次抽样一条身高数据，同时也是从男生女生分布中抽样了一个样本。

**一条身高数据样本<--->一个男生女生分布中的样本**

唯一的不同，作为身高数据样本，我们是可以看到样本的具体值的，也就是上面的身高数字，但是当将该抽样作为男生女生分布中的样本时，我们是看不到样本的具体值的，即不知道该样本到底是来自男生还是女生，这就是“隐”的含义。

我们记身高数据分布为$P(X)$，男生女生分布为$P(Z)$。
将$P(X)$称为观测变量，$P(Z)$称为隐变量。
观测变量可以由隐变量生成，即：

$$
P(X) = \sum_{Z} P(X, Z) = \sum_{Z} P(Z)P(X|Z)
$$

理解这个公式，我们可以使用上面男生女生身高的例子。假设男生身高分布为$P(X|Z=boy)$，女生身高分布为$P(X|Z=girl)$。
那么，$P(X)$就是男生女生身高分布的混合。
即：

$$
P(X) = P(Z=boy) P(X|Z=boy) + P(Z=girl) P(X|Z=girl)
$$

上面的公式可以类比全概率公式来理解。





















但是没办法确定一条具体的数据是来自男生还是女生，因为我们只有一个冷冰冰的身高数字。但是


如果假设这些数据是来自一个正态分布，那么均值$\mu$应该在$175$附近，而协方差$\Sigma$应该在$1$附近。

下面，我们将分为两个步骤来进行我们的推导。首先就是为什么需要使用隐变量。其次，使用了隐变量后，为什么需要使用EM算法。
很多文章中这两个问题是一笔带过，没有详细推导。好像使用了隐变量，就必须用EM算法。实际上，并不是这样的。


但是实际上，咱们这个假设$X$为正态分布，很多时候不好使。
比如，数据是来自多个高斯分布的混合。这里所谓的混合，可以理解成一条数据$x$的概率$P(x)$，是由多个高斯分布混合而成的。每个高斯混合分布贡献一部分概率。不妨假设有$K$个高斯分布，记为$G_1, G_2, \ldots, G_K$。
$P(x)$可以表示为：

$$
P(x) = \sum_{i=1}^K \alpha_i P(x|G_i)
$$

其中$\alpha_i$是每个高斯分布的权重，且满足$\sum_{i=1}^K \alpha_i = 1$。

这里$\alpha_i$是和具体的$x$相关的。每个$x$的$\alpha_i$的和为1。




















