---
layout: post
title:  "理解VAE算法"
date:   2024-12-03 09:20:08 +0800
category: "AI"
published: true
---

本文将从上篇文章[理解EM算法]({% post_url 2024-11-18-理解EM算法 %})出发，继续推导出$VAE$算法。

<!--more-->

## 回顾EM算法
让我们回顾一下，我们得到$EM$算法的关键步骤如下。首先，我们将log-likelihood分解成了联合分布和后验分布。

$$
\log P(x|\theta) = \log P(x,z|\theta) - \log P(z|x, \theta)  \tag{1}
$$

然后通过在等式两边对$\theta^{(n)}$时得到的后验分布求期望，得到：
$$
\begin{aligned}
\sum_{z} P(z|x, \theta^{(n)})\log P(x|\theta) &= \sum_{z} P(z|x, \theta^{(n)}) \log \frac{P(x,z|\theta)}{P(z|x, \theta^{(n)})} \\
&- \sum_{z} P(z|x, \theta^{(n)}) \log \frac{P(z|x, \theta)}{P(z|x,\theta^{(n)})} 
\end{aligned}
\tag{2}
$$

稍加整理，就得到：

$$
\log P(x|\theta) = \underbrace{E_{z \in P(z|x, \theta^{(n)})} \log \frac{P(x,z|\theta)}{P(z|x, \theta^{(n)})}}_{ELBO} + KL(P(z|x, \theta^{(n)}), P(z|x, \theta)) \tag{3}
$$

# 为什么使用$\theta^{n}$时得到的后验分布
上篇文章中，我们的解释是这样可以形成$KL$这个非负项，从而得到$ELBO$下界。
但是仅这样解释，还不够有力。根据公式3，我们可以看到，$KL$越小，$ELBO$越大，也就是log-likelihood的下界越紧。
下界当然越紧越好。因为$P(z \vert x, \theta^{(n)})$是$n$次迭代时得到的，所以认为它和$P(z \vert x, \theta)$会比较接近，$KL$会比较小，这才是相对完整的解释。

## VAE
如果目的是让$KL$比较小，那么直接用$P(z \vert x, \theta)$不是最好？后面的$KL$直接就是0。
$ELBO$变成：

$$
ELBO = \sum_{z} P(z|x, \theta) \log \frac{P(x,z|\theta)}{P(z|x, \theta)} \tag{4}
$$

当隐变量$z$是高维连续变量时，$ P(z \vert x,\theta)$ 的计算涉及到对$z$的积分，计算复杂度较高，导致不可行。那怎么办？

干脆，我们用一个神经网络拟合吧。我们用$Q(z \vert x, \phi)$来拟合$P(z \vert x,\theta)$，于是公式4变成：

$$
ELBO = \sum_{z} Q(z|x, \phi) \log \frac{P(x,z|\theta)}{Q(z|x, \phi)} \tag{5}
$$


这里，我们我们要稍加停留，体会一下这个拟合。简单说，我们是用<font color="blue">$\phi$来拟合一个本应由$\theta$计算得来的分布</font>。当$\theta$改变，$\phi$也应当跟着改变。所以二者要同时进行优化，不能分开优化。

我们对公式5推导，得到：

$$
\begin{aligned}
ELBO &= \sum_{z} Q(z|x, \phi) \log \frac{P(x,z|\theta)}{Q(z|x, \phi)} \\
&=  \sum_{z} Q(z|x, \phi) \log \frac{P(x|z, \theta)P(z|\theta)}{Q(z|x, \phi)} \\
&= E_{Q(z|x, \phi)} \log P(x|z, \theta) - KL(Q(z|x, \phi), P(z|\theta))
\end{aligned}
\tag{6}
$$

这里的$KL(Q(z \vert x, \phi), P(z \vert \theta))$，当我们认为$P(z \vert \theta)$是标准正态分布时，就是$VAE$中的$KL$项。
笔者刚学时，对$P(z \vert \theta)$怎么就突然去掉$\theta$直接假设成一个标准正态分布感到困惑。

实际上，我们从它的意义出发，就很容易理解，$P(z \vert \theta)$是我们建模的隐变量$z$的先验分布，当我们对这个分布做出了标准正态分布的假设时，我们就不需要对它建模了，也就没有$\theta$了。


$E_{Q(z \vert x, \phi)} \log P(x \vert z, \theta)$ 这里的期望，我们也不能直接使用从$Q(z \vert x, \phi)$中采样的蒙特卡洛算法，原因也是因为<font color="blue">$\phi$是一个用来拟合一个本应由$\theta$计算得来的分布</font>。采样出来的每个样本，实际上都是和$\theta$相关的。如果直接使用采样的样本，就会丢掉样本中$\theta$的信息。我们对$\theta$求的梯度就不完整了。

解决这个问题的办法，就是使用重参数化技巧。

## 重参数化技巧

重参数化技巧的核心就是从一个简单的分布中采样，然后通过一个**确定的函数**，将采样结果映射到我们需要的分布中。

假设我们有正态分布：

$$
z = \mu + \sigma \epsilon
$$

其中$\mu$和$\sigma$是正态分布的均值和标准差，$\epsilon$是均值为0，标准差为1的标准正态分布。

我们从$\epsilon \sim N(0,1)$中采样，然后通过$z = \mu + \sigma \epsilon$，将采样结果映射到$N(\mu, \sigma)$中。   

这么做的好处是，采样的样本中仍然包含了$\mu$和$\sigma$的信息，我们可以对$\mu$和$\sigma$求梯度。

对应到我们上面的$VAE$，我们假设$Q(z \vert x, \phi)$是正态分布，那么我们就可以从$N(0,1)$中采样，然后通过$z = \mu + \sigma \epsilon$，将采样结果映射到$N(\mu, \sigma)$中。

关于重参数技巧，笔者也查阅了一些资料。并非所有分布都可以使用重参数化技巧，我们这里使用的正态分布是可以的。

## 总结

至此，总算写出了笔者对$VAE$的理解。笔者认为，从$EM$算法引出$VAE$算法，保持认识的连续性，有助于理解$VAE$。





