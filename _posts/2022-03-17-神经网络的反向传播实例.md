---
layout: post
title:  "神经网络的反向传播实例"
date:   2022-03-17 09:20:08 +0800
category: "AI"
published: true
---

> DNN中的反向传播的简明例子。


<!--more-->

### 1、标量对矩阵的求导法则

通过标量求导中的导数与微分关系，可以类推出标量对矩阵求导的微分表达式：

$$
df = \sum_{i=1}^m\sum_{j=1}^n \frac{\partial f}{\partial X_{ij}} = tr\bigg( \frac{\partial f^T}{\partial X} dX\bigg)
$$

上式中，f是标量，X是m\*n的矩阵。第一个是全微分公式，第二个是利用了迹的运算法则形式化之后的。

**这样形式化后之后，就可以利用微分运算进行求导。
核心逻辑是求出df的表达式，然后给它套上迹，整理变形成右边的形式，则$\frac{\partial f}{\partial X}$就是所求。**

在求解过程中，有8条微分法则和5条迹运算法则需要严格遵守。列出如下：

1、加减法：$d(X\pm Y)=dX \pm dY$

2、矩阵乘：$d(X Y)=(dX)Y + XdY$

3、转置： $d(X^T)=(dX)^T$

4、迹： $dtr(X) = tr(dX)$

5、逆：$dX^{-1} = -X^{-1}dXX^{-1}$

6、行列式：$d\lvert X \rvert = tr(X^{b}dX)$

其中，$X^{b}$表示$X$的伴随矩阵，若$X$可逆，还可以写成：

$d\lvert X \rvert=\lvert X \rvert tr(X^{-1}dX)$

7、逐元素乘法：$d(X\odot Y）=dX \odot Y + X \odot dY$

8、逐元素函数：$d\sigma (X) = \sigma^{'}(X) \odot dX$

下面是需要的迹技巧：

9、标量套上迹：$a=tr(a)$

10、转置：$tr(A^T) = tr(A)$

11、线性：$tr(A\pm B)=tr(A) \pm tr(B)$

12、 矩阵乘法交换：$tr(AB)=tr(BA)$，其中$A$与$B^T$尺寸相同。

13、矩阵乘法/逐元素乘法交换：$tr(A^T(B\odot C)) = tr((A\odot B)^TC$，其中的三个矩阵尺寸相同。


有了上面的规则，**若标量函数f是矩阵X经过加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对f求微分，再使用迹技巧给df套上迹并将其它项交换至dX左侧，对照导数与微分的联系：**


$$
df = \sum_{i=1}^m\sum_{j=1}^n \frac{\partial f}{\partial X_{ij}} = tr\bigg( \frac{\partial f^T}{\partial X} dX\bigg)
$$

即能得到导数。


### 2、例子一，交叉熵函数
$l=-y^Tlogsoftmax(a)$，求$\frac{\partial l}{\partial a}$

a 是m\*1向量。

$softmax(a)=\frac{exp(a)}{\mathbf{1}^Texp(a)}$
粗体的1代表全1向量。

$$
l = -y^T(log(exp(a)) - \mathbf{1}log(\mathbf{1}^Texp(a)) 



$$

$$
dl = -y^Tda + \frac{\mathbf{1}^Texp(a) \odot(da)}{\mathbf{1}^Texp(a)}
$$

套上迹并做交换，注意这里要利用到等式$\mathbf{1}^T(u\odot v) = u^Tv$

$$
dl = tr\bigg(-y^Tda + \frac{exp(a)^Tda}{\mathbf{1}^Texp(a)}\bigg)
$$

$$
= tr\bigg(-y^Tda + softmax(a)^Tda\bigg)
$$

$$
= tr\bigg((softmax(a)-y)^Tda \bigg)
$$

根据微分和导数的关系，可知：

$$
\frac{\partial l}{\partial a} = (softmax(a)-y)
$$

### 3、例子二：两层神经网络

$l=-y^Tlogsoftmax(W_2\sigma(W_1x))$，求$\frac{\partial l}{\partial W_1}和\frac{\partial l}{\partial W_2}$


定义：$a_1=W_1x, h_1=\sigma(a_1), a_2 = W_2h_1$，则有$l = -y^Tlogsoftmax(a_2)$，根据例子一中的结论，有：

$$
\frac{\partial l}{\partial a_2} = (softmax(a_2)-y)
$$
进而有：
$$
dl = tr\bigg(\frac{\partial l^T}{\partial a_2}da_2\bigg)
$$
$$
=tr\bigg(\frac{\partial l^T}{\partial a_2} dW_2h_1\bigg) + \underbrace{tr\bigg(\frac{\partial l^T}{\partial a_2}W_2dh_1\bigg)}_{dl_2}
$$

根据矩阵乘法交换的迹技巧，从第一项可得：

$$
\frac{\partial l}{\partial W_2} = \frac{\partial l}{\partial a_2}h_1^T
$$

从第二项可得

$$
\frac{\partial l}{\partial h_1} = W_2^T\frac{\partial l}{\partial a_2}
$$

下面对第二项继续使用复合法则求$\frac{\partial l}{\partial a_1}$。
要用到矩阵乘法和逐元素乘法的迹技巧。

$$
dl_2 = tr\bigg(\frac{\partial l ^T}{\partial h_1}(\sigma^{'}(a_1) \odot da_1)\bigg) 
$$

$$
 = tr\bigg(\big(\frac{\partial l}{\partial h_1} \odot \sigma^{'}(a_1)\big)^Tda_1\bigg) 
$$

由此得到：

$$
\frac{\partial l}{\partial a_1} = \frac{\partial l}{\partial h_1} \odot\sigma^{'}(a_1)
$$

为了求得$\frac{\partial l}{\partial W_1}$，再一次使用复合法则：

$$
dl_2 = tr\bigg(\frac{\partial l^T}{\partial a_1}da_1\bigg)=tr\bigg(\frac{\partial l^T}{\partial a_1}dW_1x\bigg)
$$
$$
=tr\bigg(x\frac{\partial l^T}{\partial a_1}dW_1\bigg)
$$

得到：

$$
\frac{\partial l}{\partial W_1} = \frac{\partial l}{\partial a_1}x^T
$$


Refs:

[矩阵求导术](https://zhuanlan.zhihu.com/p/24709748)