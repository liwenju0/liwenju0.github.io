---
layout: post
title:  "bert推理优化之路"
date:   2022-04-06 09:20:08 +0800
category: "AI"
published: true
---
之前部署上线了学员课程评论情感分析模型，随着业务发展，推理性能逐渐成为瓶颈。所以准备转到GPU进行部署，彻底解决这个瓶颈问题。
以下是当初进行推理优化时不同技术选型的测试过程。


# 1、cpu推理性能测量

目前是使用onnxruntime的cpu版本部署的。经过测量，性能如下：

batch size 为1时，预测耗时为90ms左右。

batch size 为10时，预测耗时为600ms左右。

这就是优化的起点了。
<!--more-->

# 2、 使用onnxruntime gpu进行加速
将服务部署到gpu服务器上，使用onnxruntime gpu版本进行推理，性能果然大大提升，实测结果如下：

batch size 为1时，预测耗时为8ms左右。

batch size 为10时，预测耗时为40ms左右。

性能提升了11-15倍。


# 3、使用tensorrt fp32进行加速
nvidia提供的tensorrt可以有效加速模型在gpu上的推理速度，官方宣传性能爆表，究竟是不是真的这样，实测一下才知道。

batch size 为1时，预测耗时为5ms左右。

batch size 为10时，预测耗时为30ms左右。

相比onnxruntime gpu版本，性能确实有所提升，但显然提升是有限的。

# 4、使用tensorrt fp16进行加速
为了进一步提升性能，尝试了fp16。实测结果如下：

batch size 为1时，预测耗时为2ms左右。

batch size 为10时，预测耗时为10ms左右。

这个效果确实有点惊艳了。

实际测试，使用fp16后，精确度仍然是可以保证的。因此，确定使用这个方案进行部署。再也不用为性能担心了。