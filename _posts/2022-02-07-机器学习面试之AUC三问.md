---
layout: post
title:  "机器学习面试之AUC三问"
date:   2022-02-07 15:54:08 +0800
categories: ml_foundation new
---

本文的目的是解答如下三个问题，看文章的你，如果觉得这三个问题很清楚，就不用浪费时间看下去了。如果你觉得能大概回答一下，但是详细说又有点为难，就继续看下去吧。

- 为什么ROC比PR对不均衡样本更鲁棒？
- 什么时候选择PR，什么时候选择ROC？
- 为什么AUC值一定是大于0.5的？

# 一、预备知识
为了上述问题，需要大家对ROC PR曲线有基本的认识，由于不是本文重点，仅给出一些参考资料。
关于绘制roc曲线的具体方法，请参考sklearn的文档。里面有很具体的例子，建议修改里面的数据，自己尝试一下。
[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html?highlight=roc#sklearn.metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html?highlight=roc#sklearn.metrics.roc_curve)
[https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py)
点开上面的网址后，拉到最下面，你会看到：
![image.png](/images/posts/ml_foundation/zero.png)
下载下来代码就可以玩耍了。下面提供的PR资料也是一样的。
如果你懒得点开这个网址，我这里贴出曲线图，以便有一个直观的印象：
![image.png](/images/posts/ml_foundation/one.png)

同样的，关于PR曲线，也请参考sklearn文档给出的例子：
[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html?highlight=pr#sklearn.metrics.precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html?highlight=pr#sklearn.metrics.precision_recall_curve)
[https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py)
这里给出一个图示：
![image.png](/images/posts/ml_foundation/two.png)

**给出sklearn的资料，其实我是想说明一下，寻找高质量的学习资料是很关键的。有时候，某个知识点不是难，而是你没有找到那份对你口味的好资料而已。**

这里，默认大家已经比较熟悉下面的概念了：

- Precision
- Recall
- TPR
- FPR
# 二、为什么ROC比PR对不均衡样本更鲁棒？

我们举一个例子来说明这个问题。假设一个二分类的任务，我们训练出来两个模型A和B。现在想比较这两个模型的优劣，自然是找份测试集来测试了。假设测试集中有100个正样本，1000000个负样本。

对模型A，测试结果如下：

- 100个样本被预测为正， 90个是正确的

对模型B，测试结果如下：

- 2000个样本被预测为正，同样是90个是正确的

现在我们分别计算两个模型的

- Precision
- Recall
- TPR
- FPR

|  | **模型A** | **模型B** |
| --- | --- | --- |
| **TPR** | 0.9 | 0.9 |
| **Recall** | 0.9 | 0.9 |
| **FPR** | 0.00001 | 0.00191 |
| **Precision** | 0.9 | 0.045 |
|  |  |  |


我们看到两个模型的TPR和Recall是一样的。
**关键是FPR和Precision！两个模型的FPR的差异是0.0019，但是它们的Precision差异竟然高达0.855！**
**
**我们知道，当我们画ROC和PR曲线时，我们是分别以（FPR， TPR）和（Precision，Recall）绘点描线的。
**
**对于模型A和B，他们的ROC曲线差异是比较小的，因为在同样的TPR的情况下，它们的FPR差别比较小。但是它们的PR曲线差异是比较大的，因为在同样的Recall的情况下，它们的Precision差异是相当的大。0.885意味着什么？可以看看上面的PR图，整个横坐标最长才是1！之所以有上面这样的差异，原因在于我们的样本分布不均衡。**
**
至此，我们通过简单的例子，就说明了问题。

# 三、什么时候选择PR，什么时候选择ROC？

**本质上，第一问中的差异在于ROC和PR关注的点是不一样的。**ROC是同时关注对正负样本，PR只关注正样本。这个不难理解，因为**TPR是衡量正样本，FPR是衡量负样本。但是，Precision和Recall都是衡量的正样本。**
**
举例来说，对于预测癌症，我们会更喜欢PR，因为我们希望尽可能准的同时，尽可能多的预测出癌症患者，不要漏掉任何一个癌症患者。至于FPR，其实不那么重要，因为总是可以通过其他更多的手段进一步核实。
但是对于猫和狗的图片分类模型，我们会更喜欢ROC，因为将猫和狗都识别准确都是我们的目标，ROC同时关注了这两方面。

# 四、为什么AUC值一定是大于0.5的？
因为当AUC小于0.5时，只要将所有的预测概率P换成1-P，就可以让AUC大于0.5了。这是一道抖机灵题哈。

# 五、无总结，不进步
在这篇文章中，主要比较了ROC和PR的区别和联系，属于很基础的知识，在实际的面试中，这些点也是经常被问到的。如果这些地方不能清晰准确地回答出面试官的问题，很容易给人留下调包侠的印象，对求职十分不利。
同时，借这个问题，分享了我关于学习知识的一个观点，**寻找优质的学习资料绝对是第一位的！**从我身边来看，很多人对这一点重视不够，希望给大家提个醒吧！

**References:**

- [https://www.kaggle.com/general/7517](https://www.kaggle.com/general/7517) 
- [https://blog.csdn.net/qq_21997625/article/details/86249444](https://blog.csdn.net/qq_21997625/article/details/86249444)
